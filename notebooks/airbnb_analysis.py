# -*- coding: utf-8 -*-
"""airbnb-analysis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hYefdVzqrCKW3KNf5HFrvfRsxAZIA0Ej
"""

#set up the instructions for python

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import PowerTransformer

#mount the drive

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/MyDrive/listings.csv')
df.head()

# IMPORTANT: How to access the shared Airbnb dataset in Colab
#
# 1. Go to Google Drive in your browser.
# 2. Locate the shared folder (e.g., "5604_group") that contains listings.csv.
# 3. Right-click → "Add shortcut to Drive" → choose "My Drive".
#    (This ensures that everyone has the same path in their own Drive.)
#
# 4. Then use the unified path to read the dataset:
#        df = pd.read_csv('/content/drive/MyDrive/5604_group/listings.csv')
#
# Please Do NOT use '/content/sample_data/' because it is temporary and user-specific.

df.info()

print("Number of rows and columns:", df.shape)

##this is to show the number of rows and columns

df.dtypes

#drop rows where price (target variable) is null
df = df.dropna(subset=['price'])

"""**Columns 0 through 26**"""

#convert column to date and just extract the year
df['host_since'] = pd.to_datetime(df['host_since'])
df['host_since'] = df['host_since'].dt.year

#create a binary variable if the host about column is null or not
df['host_about'] = df['host_about'].isnull().astype(int)

#impute nulls with "f"
df['host_is_superhost'].fillna('f', inplace=True)
df['host_has_profile_pic'].fillna('f', inplace=True)
df['host_identity_verified'].fillna('f', inplace=True)

#Impute nulls with median of the column
median_listings_count = df['host_listings_count'].median()
df['host_listings_count'].fillna(median_listings_count, inplace=True)

median_total_listings_count = df['host_total_listings_count'].median()
df['host_total_listings_count'].fillna(median_total_listings_count, inplace=True)

median_host_since = df['host_since'].median()
df['host_since'].fillna(median_host_since, inplace=True)


#Impute nulls with mode of the column
mode_response_time = df['host_response_time'].mode()[0]
df['host_response_time'].fillna(mode_response_time, inplace=True)

mode_verifications = df['host_verifications'].mode()[0]
df['host_verifications'].fillna(mode_verifications, inplace=True)

# Convert 'host_response_rate' to numeric
df['host_response_rate'] = df['host_response_rate'].str.replace('%', '').astype(float) / 100


# Define bins and labels
bins = [0, 0.2, 0.5, 0.8, 1.0]
labels = ['0-20%', '21-50%', '51-80%', '81-100%']


# Bin the numeric response rate, including NaNs
df['host_response_rate'] = pd.cut(df['host_response_rate'], bins=bins, labels=labels, right=True, include_lowest=True)


# Fill NaNs in the binned column with a specific category (e.g., 'Unknown')
df['host_response_rate'] = df['host_response_rate'].cat.add_categories('Unknown').fillna('Unknown')


# Display the value counts of the new binned column
print("Value counts for binned host_response_rate:")
display(df['host_response_rate'].value_counts())

# Convert 'host_response_rate' to numeric
df['host_acceptance_rate'] = df['host_acceptance_rate'].str.replace('%', '').astype(float) / 100


# Define bins and labels
bins = [0, 0.2, 0.5, 0.8, 1.0]
labels = ['0-20%', '21-50%', '51-80%', '81-100%']


# Bin the numeric response rate, including NaNs
df['host_acceptance_rate'] = pd.cut(df['host_acceptance_rate'], bins=bins, labels=labels, right=True, include_lowest=True)


# Fill NaNs in the binned column with a specific category (e.g., 'Unknown')
df['host_acceptance_rate'] = df['host_acceptance_rate'].cat.add_categories('Unknown').fillna('Unknown')


# Display the value counts of the new binned column
print("Value counts for binned host_acceptance_rate:")
display(df['host_acceptance_rate'].value_counts())

# Define the word to search for
search_word = 'spacious'

# Create the binary column
# The .astype(int) converts True/False to 1/0
df['description'] = df['description'].fillna('Unknown').str.contains(search_word, case=False).astype(int)

print(df['description'].value_counts())

# Define the words to search for
search_words_2 = ['coast','shore','beach']

# Create the binary column
# Check if any of the words in the list are present in the 'neighborhood_overview' column
df['neighborhood_overview'] = df['neighborhood_overview'].fillna('').str.contains('|'.join(search_words_2), case=False).astype(int)

print(df['neighborhood_overview'].value_counts())

# Define the word to search for
search_word_3 = 'New South Wales, Australia'

# Create the binary column
# The .astype(int) converts True/False to 1/0
df['host_location'] = df['host_location'].fillna('Unknown').str.contains(search_word_3, case=False).astype(int)

print(df['host_location'].value_counts())

# List of columns to drop
columns_to_drop = ['id', 'listing_url','scrape_id','last_scraped','source','picture_url','name','host_id','host_url',
                      'host_name','host_thumbnail_url','host_picture_url','host_neighbourhood']

# Drop multiple columns
df = df.drop(columns=columns_to_drop)

"""**Coloumns 53-79**"""

# Convert 'last_review' to datetime objects, coercing errors
df['last_review'] = pd.to_datetime(df['last_review'], errors='coerce')

# Define the current date
current_date = pd.to_datetime('2025-09-17')

# Calculate the difference in days and convert to months, coercing errors to NaN
df['months_since_last_review'] = (current_date - df['last_review']).dt.days / 30.44  # Approximate days in a month

# Display the first few rows of the new column and its value counts
print("Months since last review:")
display(df[['last_review', 'months_since_last_review']].head())

print("\nValue counts for months_since_last_review (showing some values):")
display(df['months_since_last_review'].value_counts().head())

#Impute nulls with median of the column
median_months_since_last_review = df['months_since_last_review'].median()
df.loc[:, 'months_since_last_review'] = df['months_since_last_review'].fillna(median_months_since_last_review)

# Impute nulls in 'reviews_per_month' with the median of the column
median_reviews_per_month = df['reviews_per_month'].median()
df['reviews_per_month'] = df['reviews_per_month'].fillna(median_reviews_per_month)

# Convert 'instant_bookable' into a binary variable (1 = yes, 0 = no)
df['instant_bookable'] = df['instant_bookable'].map({'t': 1, 'f': 0})
display(df['instant_bookable'].value_counts())

# Convert 'license' into a binary variable (1 = has license, 0 = no license)
df['license'] = df['license'].apply(lambda x: 0 if x == 'Exempt' else 1)
display(df['license'].value_counts())

columns_to_drop_2 = ['estimated_revenue_l365d', 'estimated_occupancy_l365d', 'availability_eoy', 'number_of_reviews_ly', 'calculated_host_listings_count_shared_rooms', 'first_review', 'calendar_last_scraped','last_review']

df = df.drop(columns=columns_to_drop_2)

# show the structure of scores

score_cols = [
    "review_scores_rating",
    "review_scores_accuracy",
    "review_scores_cleanliness",
    "review_scores_checkin",
    "review_scores_communication",
    "review_scores_location",
    "review_scores_value"
]

fig, axes = plt.subplots(4, 2, figsize=(14, 12))
axes = axes.flatten()

for i, col in enumerate(score_cols):
    axes[i].hist(df[col].dropna(), bins=50, color='skyblue', edgecolor='black')
    axes[i].set_title(f"{col}")
    axes[i].set_xlim(4.0, 5.0)
    axes[i].set_xticks(np.arange(4.0, 5.05, 0.1))
    axes[i].set_xlabel("Score")
    axes[i].set_ylabel("Count")


for j in range(len(score_cols), len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

# bin scores
bins = [0, 4.5, 4.9, 5.0]
labels = ["Low", "Mid", "High"]

for col in score_cols:
    df[col + "_binned"] = pd.cut(df[col], bins=bins, labels=labels, include_lowest=True)

# check
for col in score_cols:
    print(f"\n{col}_binned value counts:")
    print(df[col + "_binned"].value_counts())

fig, axes = plt.subplots(3, 3, figsize=(15, 10))
axes = axes.flatten()

for i, col in enumerate(score_cols):
    df[col + "_binned"].value_counts().reindex(labels).plot(
        kind="bar", ax=axes[i], color=["tomato", "gold", "seagreen"]
    )
    axes[i].set_title(f"{col} (Binned)")
    axes[i].set_xlabel("Category")
    axes[i].set_ylabel("Count")


for j in range(len(score_cols), len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

# Dummies
binned_cols = [f"{c}_binned" for c in score_cols]

# 1)  one-hot（drop_first=True
dummies = pd.get_dummies(
    df[binned_cols],
    prefix=binned_cols,
    prefix_sep="_",
    drop_first=True
)

# 2) replace original and binned
df = pd.concat([df.drop(columns=score_cols + binned_cols), dummies], axis=1)

# check
print(df.filter(like="review_scores").columns.tolist())
print(df.shape)

"""#Columns 27 to 52

27 neighborhood

DROP- similar to neighborhood_overview column but less effecient
"""

# Ensure 'df' is the DataFrame with the 'neighbourhood' column loaded
# If not already loaded, you might need to run the cell that loads the data first

if 'neighbourhood' in df.columns:
    # Replace "Neighborhood highlights" with 1 and NaN with 0
    df['neighbourhood'] = df['neighbourhood'].apply(lambda x: 1 if x == "Neighborhood highlights" else (0 if pd.isna(x) else x))

    # Change the data type to integer
    # Note: This might raise an error if there are other non-numeric values in the column
    df['neighbourhood'] = df['neighbourhood'].astype(int)

    # Display the head of the modified 'neighbourhood' column and its data type
    display(df[['neighbourhood']].head(10))
    print(f"Data type of 'neighbourhood': {df['neighbourhood'].dtype}")

else:
    print("The 'neighbourhood' column does not exist in the DataFrame.")

# Ensure 'df' is the DataFrame with the 'neighbourhood' column
if 'neighbourhood' in df.columns:
    print(df['neighbourhood'])
else:
    print("The 'neighbourhood' column does not exist in the DataFrame.")

# Ensure 'df' is the DataFrame and the column exists
if 'neighbourhood' in df.columns:
    df = df.drop('neighbourhood', axis=1)
    print("The 'neighbourhood' column has been dropped.")
else:
    print("The 'neighbourhood' column does not exist in the DataFrame.")

"""28 neighbourhood_cleansed"""

# Ensure 'df' is the DataFrame with the 'neighbourhood_cleansed' column
if 'neighbourhood_cleansed' in df.columns:
    unique_values = df['neighbourhood_cleansed'].unique()
    print("Unique values in 'neighbourhood_cleansed' column:")
    print(unique_values)
else:
    print("The 'neighbourhood_cleansed' column does not exist in the DataFrame.")

"""29 Neighbourhood_group_cleansed

DROP- 6139 missing values

"""

# Ensure 'df' is the DataFrame
if 'neighbourhood_group_cleansed' in df.columns:
    missing_values = df['neighbourhood_group_cleansed'].isnull().sum()
    print(f"Number of missing values in 'neighbourhood_group_cleansed': {missing_values}")
else:
    print("The 'neighbourhood_group_cleansed' column does not exist in the DataFrame.")

# Ensure 'df' is the DataFrame and the column exists
if 'neighbourhood_group_cleansed' in df.columns:
    df = df.drop('neighbourhood_group_cleansed', axis=1)
    print("The 'neighbourhood_group_cleansed' column has been dropped.")
else:
    print("The 'neighbourhood_group_cleansed' column does not exist in the DataFrame.")

"""30 latitude

31 longitude

DROP
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Ensure 'df' is the DataFrame and the columns exist
if 'latitude' in df.columns and 'longitude' in df.columns:
    # Create a figure with two subplots
    fig, axes = plt.subplots(1, 2, figsize=(10, 5))

    # Create a box plot for latitude
    sns.boxplot(y=df['latitude'], ax=axes[0])
    axes[0].set_title('Box Plot of Latitude')
    axes[0].set_ylabel('Latitude')

    # Create a box plot for longitude
    sns.boxplot(y=df['longitude'], ax=axes[1])
    axes[1].set_title('Box Plot of Longitude')
    axes[1].set_ylabel('Longitude')

    # Adjust layout to prevent overlapping titles/labels
    plt.tight_layout()

    # Show the plot
    plt.show()

else:
    print("The 'latitude' or 'longitude' column does not exist in the DataFrame.")

# Ensure 'df' is the DataFrame and the columns exist
columns_to_drop2 = ['latitude', 'longitude']
existing_columns_to_drop = [col for col in columns_to_drop2 if col in df.columns]

if existing_columns_to_drop:
    df = df.drop(existing_columns_to_drop, axis=1)
    print(f"The following columns have been dropped: {existing_columns_to_drop}")
else:
    print("None of the specified columns exist in the DataFrame.")

"""32 property_type

Binning it into a narrow spectrum.
"""

# Ensure 'df' is the DataFrame with the 'property_type' column
if 'property_type' in df.columns:
    unique_values = df['property_type'].unique()
    print("Unique values in 'property_type' column:")
    print(unique_values)
else:
    print("The 'property_type' column does not exist in the DataFrame.")

# Ensure 'df' is the DataFrame with the 'property_type' column
if 'property_type' in df.columns:
    # Define the mapping as provided by the user
    bins = {
        'entire_home_unit': [
            'Entire home', 'Entire rental unit', 'Entire guesthouse', 'Entire guest suite',
            'Entire townhouse', 'Entire cabin', 'Entire cottage', 'Entire villa',
            'Entire serviced apartment', 'Entire condo', 'Entire bungalow',
            'Entire vacation home', 'Entire loft', 'Entire place'
        ],
        'private_shared_room': [
            'Private room in home', 'Private room in rental unit', 'Private room in guesthouse',
            'Private room in guest suite', 'Private room in townhouse', 'Private room in cottage',
            'Private room in bed and breakfast', 'Private room in hostel',
            'Private room in farm stay'
        ],
        'unique': [
            'Tiny home', 'Camper/RV', 'Tent', 'Barn', 'Farm stay'
        ],
        'hotel': [
            'Room in hotel', 'Room in boutique hotel', 'Bed & Breakfast'
        ]
    }

    # Create a temporary column for categories
    df['property_category_temp'] = 'Other'

    # Iterate through the bins and assign categories to the temporary column
    for category, types in bins.items():
        for property_type in types:
            # Using case-insensitive matching and checking for exact match within a longer string
            df.loc[df['property_type'].str.contains(r'\b' + property_type + r'\b', case=False, na=False), 'property_category_temp'] = category

    # Replace the original 'property_type' column with the temporary category column
    df['property_type'] = df['property_category_temp']

    # Drop the temporary column
    df = df.drop('property_category_temp', axis=1)


    # Display the head of the modified 'property_type' column and the value counts
    display(df[['property_type']].head())
    print("\nValue counts of the modified 'property_type' column:")
    print(df['property_type'].value_counts())

else:
    print("The 'property_type' column does not exist in the DataFrame.")

"""33 room_type

"""

# Ensure 'df' is the DataFrame with the 'room_type' column
if 'room_type' in df.columns:
    unique_values = df['room_type'].unique()
    print("Unique values in 'room_type' column:")
    print(unique_values)
else:
    print("The 'room_type' column does not exist in the DataFrame.")

# Ensure 'df' is the DataFrame and the columns exist
if 'room_type' in df.columns and 'price' in df.columns:
    plt.figure(figsize=(10, 6))
    sns.boxplot(x='room_type', y='price', data=df)
    plt.title('Distribution of Price by Room Type')
    plt.xlabel('Room Type')
    plt.ylabel('Price')
    plt.xticks(rotation=45) # Rotate labels for better readability
    plt.tight_layout()
    plt.show()
else:
    print("The 'room_type' or 'price' column does not exist in the DataFrame.")

"""34 accommodates"""

# Ensure 'df' is the DataFrame with the 'accommodates' column
if 'accommodates' in df.columns:
    unique_values = df['accommodates'].unique()
    # Sort the unique values in ascending order
    unique_values.sort()
    print("Unique values in 'accommodates' column (ascending order):")
    print(unique_values)
else:
    print("The 'accommodates' column does not exist in the DataFrame.")

"""35 bathrooms"""

# Ensure 'df' is the DataFrame and the column exists
if 'bathrooms' in df.columns:
    print(f"Data type of 'bathrooms': {df['bathrooms'].dtype}")
else:
    print("The 'bathrooms' column does not exist in the DataFrame.")

# Ensure 'df' is the DataFrame with the 'bathrooms' column
if 'bathrooms' in df.columns:
    unique_values = df['bathrooms'].unique()
    # Sort the unique values in ascending order and handle NaN
    unique_values_sorted = sorted(unique_values, key=lambda x: (pd.isna(x), x))
    # Convert to a list
    unique_values_list = list(unique_values_sorted)
    print("Unique values in 'bathrooms' column (ascending order):")
    print(unique_values_list)
else:
    print("The 'bathrooms' column does not exist in the DataFrame.")

"""36 bathrooms_text

DROP
"""

# Ensure 'df' is the DataFrame and the column exists
if 'bathrooms_text' in df.columns:
    missing_values = df['bathrooms_text'].isnull().sum()
    print(f"Number of missing values in 'bathrooms_text': {missing_values}")
else:
    print("The 'bathrooms_text' column does not exist in the DataFrame.")

print(df['bathrooms_text'].head())

# Ensure 'df' is the DataFrame and the column exists
if 'bathrooms_text' in df.columns:
    df = df.drop('bathrooms_text', axis=1)
    print("The 'bathrooms_text' column has been dropped.")
else:
    print("The 'bathrooms_text' column does not exist in the DataFrame.")

"""37 bedrooms"""

# Ensure 'df' is the DataFrame and the column exists
if 'bedrooms' in df.columns:
    missing_values = df['bedrooms'].isnull().sum()
    print(f"Number of missing values in 'bedrooms': {missing_values}")
else:
    print("The 'bedrooms' column does not exist in the DataFrame.")


print(df['bedrooms'].dtype)
print(df['bedrooms'].head(15))

# Ensure 'df' is the DataFrame and the 'bedrooms' column exists
if 'bedrooms' in df.columns:
    initial_rows = df.shape[0]
    df.dropna(subset=['bedrooms'], inplace=True)
    rows_after_dropping = df.shape[0]
    rows_dropped_bedrooms = initial_rows - rows_after_dropping
    print(f"Dropped {rows_dropped_bedrooms} row with missing values in the 'bedrooms' column.")
    print(f"Number of rows remaining: {rows_after_dropping}")
else:
    print("The 'bedrooms' column does not exist in the DataFrame.")

import matplotlib.pyplot as plt
import seaborn as sns

# Ensure 'df' is the DataFrame and the 'bedrooms' column exists
if 'bedrooms' in df.columns:
    plt.figure(figsize=(6, 4))
    sns.histplot(df['bedrooms'], bins=df['bedrooms'].nunique(), kde=False)
    plt.title('Distribution of Bedrooms')
    plt.xlabel('Number of Bedrooms')
    plt.ylabel('Frequency')
    plt.show()
else:
    print("The 'bedrooms' column does not exist in the DataFrame.")

"""38 beds


Question-
Should we drop 662 missing value rows?
"""

print(df['beds'].dtype)
print(df['beds'].head())

# Ensure 'df' is the DataFrame and the column exists
if 'beds' in df.columns:
    missing_values = df['beds'].isnull().sum()
    print(f"Number of missing values in 'beds': {missing_values}")
else:
    print("The 'beds' column does not exist in the DataFrame.")

# Ensure 'df' is the DataFrame and the 'beds' column exists
if 'beds' in df.columns:
    initial_rows = df.shape[0]
    df.dropna(subset=['beds'], inplace=True)
    rows_after_dropping = df.shape[0]
    rows_dropped_beds = initial_rows - rows_after_dropping
    print(f"Dropped {rows_dropped_beds} rows with missing values in the 'beds' column.")
    print(f"Number of rows remaining: {rows_after_dropping}")
else:
    print("The 'beds' column does not exist in the DataFrame.")

import matplotlib.pyplot as plt
import seaborn as sns

# Ensure 'df' is the DataFrame and the columns exist
if 'bedrooms' in df.columns and 'beds' in df.columns and 'price' in df.columns:
    # Scatter plot for Bedrooms vs Price
    plt.figure(figsize=(8, 5))
    sns.scatterplot(x=df['bedrooms'], y=df['price'])
    plt.title('Scatter Plot of Bedrooms vs Price')
    plt.xlabel('Bedrooms')
    plt.ylabel('Price')
    plt.show()

    # Scatter plot for Beds vs Price
    plt.figure(figsize=(8, 5))
    sns.scatterplot(x=df['beds'], y=df['price'])
    plt.title('Scatter Plot of Beds vs Price')
    plt.xlabel('Beds')
    plt.ylabel('Price')
    plt.show()

else:
    print("The 'bedrooms', 'beds', or 'price' column does not exist in the DataFrame.")

# Ensure 'df' is the DataFrame and the columns exist
if 'bedrooms' in df.columns and 'price' in df.columns:
    # Clean the 'price' column and convert to numeric
    df['price'] = df['price'].astype(str).str.replace(r'[$,]', '', regex=True)
    df['price'] = pd.to_numeric(df['price'], errors='coerce')

    correlation_bedrooms_price = df['bedrooms'].corr(df['price'])
    print(f"The correlation between 'bedrooms' and 'price' is: {correlation_bedrooms_price}")
else:
    print("The 'bedrooms' or 'price' column does not exist in the DataFrame.")

if 'beds' in df.columns and 'price' in df.columns:
    # Clean the 'price' column and convert to numeric
    df['price'] = df['price'].astype(str).str.replace(r'[$,]', '', regex=True)
    df['price'] = pd.to_numeric(df['price'], errors='coerce')

    correlation_beds_price = df['beds'].corr(df['price'])
    print(f"The correlation between 'beds' and 'price' is: {correlation_beds_price}")
else:
    print("The 'beds' or 'price' column does not exist in the DataFrame.")

"""39 amenities

counted the list of amenities of the propert.
haven't binned it.
"""

# Ensure 'df' is the DataFrame with the 'amenities' column loaded
# If not already loaded, you might need to run the cell that loads the data first

if 'amenities' in df.columns:
    # Count the number of commas and add 1
    # This assumes that amenities are listed and separated by commas.
    # Adding 1 accounts for the first amenity before any comma.
    df['amenities_count'] = df['amenities'].astype(str).str.count(',') + 1

    # Replace the original 'amenities' column with the count
    df['amenities'] = df['amenities_count']

    # Drop the temporary count column
    df = df.drop('amenities_count', axis=1)

    # Ensure the data type is integer
    df['amenities'] = df['amenities'].astype(int)

    # Display the head of the modified 'amenities' column and its data type
    display(df[['amenities']].head())
    print(f"Data type of 'amenities': {df['amenities'].dtype}")

else:
    print("The 'amenities' column does not exist in the DataFrame.")

"""we could bin it

40 price
"""

df['price'] = df['price'].astype(str).str.replace(r'[$,]', '', regex=True)
df['price'] = pd.to_numeric(df['price'], errors='coerce')
print(df['price'])

# Ensure 'df' is the DataFrame and the column exists
if 'price' in df.columns:
    missing_values = df['price'].isnull().sum()
    print(f"Number of missing values in 'price': {missing_values}")
else:
    print("The 'price' column does not exist in the DataFrame.")

# Ensure 'df' is the DataFrame and the 'price' column exists
if 'price' in df.columns:
    initial_rows = df.shape[0]
    df.dropna(subset=['price'], inplace=True)
    rows_after_dropping = df.shape[0]
    rows_dropped_price = initial_rows - rows_after_dropping
    print(f"Dropped {rows_dropped_price} rows with missing values in the 'price' column.")
    print(f"Number of rows remaining: {rows_after_dropping}")
else:
    print("The 'price' column does not exist in the DataFrame.")

"""41 minimum_nights

"""

import matplotlib.pyplot as plt
import seaborn as sns

# Ensure 'df' is the DataFrame and the columns exist
if 'minimum_nights' in df.columns and 'price' in df.columns:
    # Calculate the correlation between 'minimum_nights' and 'price'
    correlation = df['minimum_nights'].corr(df['price'])
    print(f"The correlation between 'minimum_nights' and 'price' is: {correlation}")

    # Create a scatter plot to visualize the relationship
    plt.figure(figsize=(6, 4))
    sns.scatterplot(x=df['minimum_nights'], y=df['price'])
    plt.title('Scatter Plot of Minimum Nights vs Price')
    plt.xlabel('Minimum Nights')
    plt.ylabel('Price')
    plt.show()

else:
    print("The 'minimum_nights' or 'price' column does not exist in the DataFrame.")

"""The scatter plot shows a very weak positive linear relationship between 'minimum_nights' and 'price'.

42 maximum_nights
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Ensure 'df' is the DataFrame and the columns exist
if 'maximum_nights' in df.columns and 'price' in df.columns:
    # Calculate the correlation between 'maximum_nights' and 'price'
    correlation = df['maximum_nights'].corr(df['price'])
    print(f"The correlation between 'maximum_nights' and 'price' is: {correlation}")

    # Create a scatter plot to visualize the relationship
    plt.figure(figsize=(6, 4))
    sns.scatterplot(x=df['maximum_nights'], y=df['price'])
    plt.title('Scatter Plot of Maximum Nights vs Price')
    plt.xlabel('Maximum Nights')
    plt.ylabel('Price')
    plt.show()

else:
    print("The 'maximum_nights' or 'price' column does not exist in the DataFrame.")

"""The correlation between 'maximum_nights' and 'price' is very weak, indicating almost no linear relationship.

43 Minimum_minimum_nights,

44 Maximum_minimum_nights,

45 Minimum_maximum_nights,

46 Maximum_maximum_nights,

47 Minimum_nights_avg_ntm,

48 Maximum_nights_avg_ntm



DROP- Either redundant with other variables or derived fields with low predictive value.
"""

columns_to_drop1 = [ 'minimum_nights',
                    'maximum_nights',
    'minimum_minimum_nights',
    'maximum_minimum_nights',
    'minimum_maximum_nights',
    'maximum_maximum_nights',
    'minimum_nights_avg_ntm',
    'maximum_nights_avg_ntm'
]

# Ensure 'df' is the DataFrame and the columns exist before dropping
existing_columns_to_drop = [col for col in columns_to_drop1 if col in df.columns]

if existing_columns_to_drop:
    df = df.drop(existing_columns_to_drop, axis=1)
    print(f"The following columns have been dropped: {existing_columns_to_drop}")
else:
    print("None of the specified columns exist in the DataFrame.")

"""49 calendar_updated

DROP- 6139 missing values

"""

# Ensure 'df' is the DataFrame
if 'calendar_updated' in df.columns:
    missing_values = df['calendar_updated'].isnull().sum()
    print(f"Number of missing values in 'calendar_updated': {missing_values}")
else:
    print("The 'calendar_updated' column does not exist in the DataFrame.")

# Ensure 'df' is the DataFrame and the column exists
if 'calendar_updated' in df.columns:
    df = df.drop('calendar_updated', axis=1)
    print("The 'calendar_updated' column has been dropped.")
else:
    print("The 'calendar_updated' column does not exist in the DataFrame.")

"""
50 has_availability

DROP- can drop the column as it doesn't have a lot of unavailability."""

import numpy as np

# Ensure 'df' is the DataFrame with the 'has_availability' column
if 'has_availability' in df.columns:
    # Replace 't' with 1 and NaN with 0, leaving other values (if any) as they are
    df['has_availability'] = np.where(df['has_availability'] == 't', 1, np.where(df['has_availability'].isna(), 0, df['has_availability']))

    # Convert the column to integer type
    df['has_availability'] = df['has_availability'].astype(int)

    # Display the head of the modified 'has_availability' column and its data type
    display(df[['has_availability']].head())
    print(f"Data type of 'has_availability': {df['has_availability'].dtype}")
    print("\nValue counts of the modified 'has_availability' column:")
    print(df['has_availability'].value_counts())

else:
    print("The 'has_availability' column does not exist in the DataFrame.")

# Ensure 'df' is the DataFrame and the column exists
if 'has_availability' in df.columns:
    df = df.drop('has_availability', axis=1)
    print("The 'has_availability' column has been dropped.")
else:
    print("The 'has_availability' column does not exist in the DataFrame.")

"""
51 availability_30
"""

# Ensure 'df' is the DataFrame and the columns exist
if 'availability_30' in df.columns and 'price' in df.columns:
    # Calculate the correlation between 'availability_30' and 'price'
    correlation = df['availability_30'].corr(df['price'])
    print(f"The correlation between 'availability_30' and 'price' is: {correlation}")

    # Create a scatter plot to visualize the relationship
    plt.figure(figsize=(6, 4))
    sns.scatterplot(x=df['availability_30'], y=df['price'])
    plt.title('Scatter Plot of Availability 30 vs Price')
    plt.xlabel('Availability 30')
    plt.ylabel('Price')
    plt.show()

else:
    print("The 'availability_30' or 'price' column does not exist in the DataFrame.")

"""The scatter plot shows no strong linear relationship between 'availability_30' and 'price'.

52 availability_60
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Ensure 'df' is the DataFrame and the columns exist
if 'availability_60' in df.columns and 'price' in df.columns:
    # Calculate the correlation between 'availability_60' and 'price'
    correlation = df['availability_60'].corr(df['price'])
    print(f"The correlation between 'availability_60' and 'price' is: {correlation}")

    # Create a scatter plot to visualize the relationship
    plt.figure(figsize=(6, 4))
    sns.scatterplot(x=df['availability_60'], y=df['price'])
    plt.title('Scatter Plot of Availability 60 vs Price')
    plt.xlabel('Availability 60')
    plt.ylabel('Price')
    plt.show()

else:
    print("The 'availability_60' or 'price' column does not exist in the DataFrame.")

"""The scatter plot shows no strong linear relationship between 'availability_60' and 'price'."""

# Ensure 'df' is the DataFrame and the columns exist
if 'number_of_reviews' in df.columns and 'price' in df.columns:
    correlation_reviews_price = df['number_of_reviews'].corr(df['price'])
    print(f"The correlation between 'number_of_reviews' and 'price' is: {correlation_reviews_price}")
else:
    print("The 'number_of_reviews' or 'price' column does not exist in the DataFrame.")

# Ensure 'df' is the DataFrame and the column exists
if 'number_of_reviews' in df.columns:
    df = df.drop('number_of_reviews', axis=1)
    print("The 'number_of_reviews' column has been dropped.")
else:
    print("The 'number_of_reviews' column does not exist in the DataFrame.")

# Ensure 'df' is the DataFrame and the columns exist
if 'number_of_reviews_l30d' in df.columns and 'price' in df.columns:
    correlation_reviews_l30d_price = df['number_of_reviews_l30d'].corr(df['price'])
    print(f"The correlation between 'number_of_reviews_l30d' and 'price' is: {correlation_reviews_l30d_price}")
else:
    print("The 'number_of_reviews_l30d' or 'price' column does not exist in the DataFrame.")

# Ensure 'df' is the DataFrame and the column exists
if 'number_of_reviews_l30d' in df.columns:
    df = df.drop('number_of_reviews_l30d', axis=1)
    print("The 'number_of_reviews_l30d' column has been dropped.")
else:
    print("The 'number_of_reviews_l30d' column does not exist in the DataFrame.")

columns_to_drop = ['availability_60', 'availability_90', 'number_of_reviews_ltm']

# Ensure the columns exist before dropping
existing_columns_to_drop = [col for col in columns_to_drop if col in df.columns]

if existing_columns_to_drop:
    df = df.drop(columns=existing_columns_to_drop)
    print(f"Dropped columns: {existing_columns_to_drop}")
else:
    print("None of the specified columns were found in the DataFrame.")

df.info()

## Detecting Outliers in All Numeric Columns

# Select only the numeric columns
numeric_cols = df.select_dtypes(include=np.number).columns

# Iterate through each numeric column to find outliers
for col in numeric_cols:
    print(f"Checking for outliers in column: {col}")

    # calculate interquartile range (IQR)
    q1 = df[col].quantile(0.1)
    q3 = df[col].quantile(0.9)
    iqr = q3 - q1

    # define the boundaries for an outlier
    lower_bound = q1 - (3 * iqr)
    upper_bound = q3 + (3 * iqr)

    # find the outliers
    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]

    # display the outliers for the current column
    if not outliers.empty:
        print(f"Outliers found in {col}: {len(outliers)} outliers")
        # Display only the outlier values from the current column
        print(outliers[col])
    else:
        print(f"No outliers found in {col} using the quantile range method.")
    print("-" * 30) # Separator for clarity between columns

# Correlations
# Select only the continuous variables
continuous_cols = df.select_dtypes(include=['int64', 'float64']).columns

# Create the correlation table
correlation_table = df[continuous_cols].corr()

# Display the correlation table with color
print("Correlation Table of Continuous Variables:")
display(correlation_table.style.background_gradient(cmap='coolwarm', axis=None))

# List of columns to drop
columns_to_drop_3 = ['host_listings_count','host_total_listings_count','calculated_host_listings_count']

# Drop multiple columns
df = df.drop(columns=columns_to_drop_3)

# Columns to check for outliers and drop
outlier_cols = [
    'bathrooms',
    'bedrooms',
    'beds',
    'calculated_host_listings_count_private_rooms',
    'reviews_per_month',
    'months_since_last_review'
]

# Iterate through each specified column to remove outliers
for col in outlier_cols:
    if col in df.columns:
        print(f"Removing outliers from column: {col}")

        # Calculate Q1, Q3, and IQR
        q1 = df[col].quantile(0.1)
        q3 = df[col].quantile(0.9)
        iqr = q3 - q1

        # Define outlier bounds
        lower_bound = q1 - (3 * iqr)
        upper_bound = q3 + (3 * iqr)

        # Identify and remove outliers
        initial_rows = df.shape[0]
        df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]
        rows_after_dropping = df.shape[0]
        rows_dropped = initial_rows - rows_after_dropping

        print(f"Dropped {rows_dropped} outliers in column {col}.")
        print(f"Number of rows remaining: {rows_after_dropping}")
    else:
        print(f"Column '{col}' not found in the DataFrame.")

print("\nDataFrame shape after outlier removal:", df.shape)

## Detecting Outliers in All Numeric Columns

# Select only the numeric columns
numeric_cols = df.select_dtypes(include=np.number).columns

# Iterate through each numeric column to find outliers
for col in numeric_cols:
    print(f"Checking for outliers in column: {col}")

    # calculate interquartile range (IQR)
    q1 = df[col].quantile(0.1)
    q3 = df[col].quantile(0.9)
    iqr = q3 - q1

    # define the boundaries for an outlier
    lower_bound = q1 - (3 * iqr)
    upper_bound = q3 + (3 * iqr)

    # find the outliers
    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]

    # display the outliers for the current column
    if not outliers.empty:
        print(f"Outliers found in {col}: {len(outliers)} outliers")
        # Display only the outlier values from the current column
        print(outliers[col])
    else:
        print(f"No outliers found in {col} using the quantile range method.")
    print("-" * 30) # Separator for clarity between columns

# Print the 'price' column
print(df['price'])

"""#MAHALANOBIS"""

from scipy.spatial.distance import mahalanobis
from sklearn.preprocessing import StandardScaler
import numpy as np
import pandas as pd

# Select only numeric columns for Mahalanobis distance calculation, excluding boolean columns
numeric_cols_for_mahalanobis = df.select_dtypes(include=np.number).columns.tolist()
boolean_cols = df.select_dtypes(include=bool).columns.tolist()
numeric_cols_for_mahalanobis = [col for col in numeric_cols_for_mahalanobis if col not in boolean_cols]

numeric_df = df[numeric_cols_for_mahalanobis].copy()

# Scale the numeric data
scaler = StandardScaler()
scaled_data = scaler.fit_transform(numeric_df)

# Calculate the covariance matrix
covariance_matrix = np.cov(scaled_data.T)

# Calculate the inverse of the covariance matrix
# Add a small value to the diagonal for numerical stability if needed
try:
    inv_covariance_matrix = np.linalg.inv(covariance_matrix)
except np.linalg.LinAlgError:
    print("Covariance matrix is still singular, adding a small value to the diagonal for stability.")
    covariance_matrix += np.eye(covariance_matrix.shape[0]) * 1e-6
    inv_covariance_matrix = np.linalg.inv(covariance_matrix)


# Calculate Mahalanobis distance for each row
mahalanobis_distances = []
for i in range(scaled_data.shape[0]):
    md = mahalanobis(scaled_data[i, :], np.mean(scaled_data, axis=0), inv_covariance_matrix)
    mahalanobis_distances.append(md)

# Add Mahalanobis distance to the DataFrame
df['mahalanobis_distance'] = mahalanobis_distances

# Define a threshold for outliers (you might need to adjust this based on your data)
# A common approach is to use a chi-squared distribution critical value
from scipy.stats import chi2
alpha = 0.001  # significance level (more strict for outlier detection)
degrees_of_freedom = scaled_data.shape[1]
threshold = chi2.ppf(1 - alpha, degrees_of_freedom)

# Identify outliers
outliers_mahalanobis = df[df['mahalanobis_distance'] > threshold]

print(f"Number of potential row-level outliers found using Mahalanobis distance (alpha={alpha}): {len(outliers_mahalanobis)}")
print("\nPotential row-level outliers (first 5):")
display(outliers_mahalanobis.head())

from sklearn.preprocessing import StandardScaler
import numpy as np

# Identify numeric columns, excluding boolean and 'mahalanobis_distance'
numeric_cols_to_standardize = df.select_dtypes(include=np.number).columns.tolist()
boolean_cols = df.select_dtypes(include=bool).columns.tolist()
exclude_cols = ['mahalanobis_distance', 'price']
numeric_cols_to_standardize = [col for col in numeric_cols_to_standardize if col not in boolean_cols and col not in exclude_cols]


# Create a StandardScaler instance
scaler = StandardScaler()

# Apply standardization to the selected columns
df[numeric_cols_to_standardize] = scaler.fit_transform(df[numeric_cols_to_standardize])

print("Numeric columns standardized (excluding boolean, Mahalanobis distance, and price):")
print(numeric_cols_to_standardize)
display(df[numeric_cols_to_standardize].head())

# Save the DataFrame after Mahalanobis distance calculation to a CSV file
df.to_csv('df_with_mahalanobis_distance.csv', index=False)

print("DataFrame with Mahalanobis distance saved as 'df_with_mahalanobis_distance.csv'")

# Print the 'price' column
print(df['price'])

"""#PCA

"""

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import numpy as np

# Select only numeric columns for PCA, excluding binary, Mahalanobis distance, and price columns
numeric_cols_for_pca = df.select_dtypes(include=np.number).columns.tolist()
boolean_cols = df.select_dtypes(include=bool).columns.tolist()
exclude_cols = ['mahalanobis_distance', 'price']

# Identify binary columns (assuming binary columns are int64 with only 0 and 1 values)
potential_binary_cols = [col for col in df.select_dtypes(include=np.number).columns if df[col].nunique() <= 2 and df[col].isin([0, 1]).all()]

cols_to_pca = [col for col in numeric_cols_for_pca if col not in boolean_cols and col not in exclude_cols and col not in potential_binary_cols]


numeric_df_for_pca = df[cols_to_pca].copy()

# Scale the numeric data
scaler = StandardScaler()
scaled_data_for_pca = scaler.fit_transform(numeric_df_for_pca)

# Perform PCA
pca = PCA()
pca.fit(scaled_data_for_pca)

# Display explained variance ratio
print("Explained variance ratio of principal components:")
print(pca.explained_variance_ratio_)

# Display cumulative explained variance ratio
cumulative_explained_variance = np.cumsum(pca.explained_variance_ratio_)
print("\nCumulative explained variance ratio:")
print(cumulative_explained_variance)

# Optionally, visualize the explained variance
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(cumulative_explained_variance) + 1), cumulative_explained_variance, marker='o', linestyle='--')
plt.title('Explained Variance by Number of Principal Components')
plt.xlabel('Number of Principal Components')
plt.ylabel('Cumulative Explained Variance Ratio')
plt.grid(True)
plt.show()

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import numpy as np
import pandas as pd

# Select numeric columns for PCA, excluding binary, Mahalanobis distance, and price columns
numeric_cols_for_pca = df.select_dtypes(include=np.number).columns.tolist()
boolean_cols = df.select_dtypes(include=bool).columns.tolist()
exclude_cols = ['mahalanobis_distance', 'price']

# Identify binary columns (assuming binary columns are int64 with only 0 and 1 values)
potential_binary_cols = [col for col in df.select_dtypes(include=np.number).columns if df[col].nunique() <= 2 and df[col].isin([0, 1]).all()]

cols_to_pca = [col for col in numeric_cols_for_pca if col not in boolean_cols and col not in exclude_cols and col not in potential_binary_cols]

numeric_df_for_pca = df[cols_to_pca].copy()

# Scale the numeric data
scaler = StandardScaler()
scaled_data_for_pca = scaler.fit_transform(numeric_df_for_pca)

# Perform PCA with 12 components
pca = PCA(n_components=12)
principal_components = pca.fit_transform(scaled_data_for_pca)

# Create a DataFrame with the principal components
pca_df = pd.DataFrame(data=principal_components, columns=[f'PC{i+1}' for i in range(12)], index=df.index)

# Select the original columns used for PCA
original_cols_for_pca_df = df[cols_to_pca].copy()

# Select the 'price' and 'mahalanobis_distance' columns
price_and_mahalanobis_df = df[['price', 'mahalanobis_distance']].copy()

# Concatenate the PCA DataFrame, original columns DataFrame, and price/mahalanobis DataFrame
new_df_with_originals = pd.concat([pca_df, original_cols_for_pca_df, price_and_mahalanobis_df], axis=1)

print("Shape of the new DataFrame:", new_df_with_originals.shape)
display(new_df_with_originals.head())

# Save the new DataFrame to a CSV file
new_df_with_originals.to_csv('new_dataframe_with_originals.csv', index=False)

print("New DataFrame saved as 'new_dataframe_with_originals.csv'")